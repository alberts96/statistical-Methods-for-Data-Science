ROCit_ldaW<- rocit(score=prob.lda$failed,class=test.transformed$status)
plot(ROCit_lda, col = c(5,'grey50'),legend=FALSE)
lines(ROCit_ldaW$TPR~ROCit_ldaW$FPR, col=6)
legend("bottomright", col = c(75,6), c("LDA", "Weighted LDA"), lwd = 2)
prob.rf
fit.lr <- train(`status`~., data=train.transformed, method="LogitBoost", metric="F1", trControl=control, weights = model_weights)
prob.lr <- predict(fit.lr, test.transformed, type ='prob')
ROCit_lrW <- rocit(score=prob.lr$failed,class=test.transformed$status)
plot(ROCit_lr, col = c(5,'grey50'),legend=FALSE)
lines(ROCit_lrW$TPR~ROCit_lrW$FPR, col=6)
legend("bottomright", col = c(75,6), c("Logit Boost", "Weighted Logit Boost"), lwd = 2)
1/table(train.data$status)[1]) * 0.5
1/table(train.data$status)[1] * 0.5
sum(model_weights)
model_weights <- ifelse(train.data$status == "failed",
(1/table(train.data$status)[1]) * 0.5,
(1/table(train.data$status)[2]) * 0.5)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="accuracy", trControl=control,weights=model_weights)
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
ROCit_ldaW<- rocit(score=prob.lda$failed,class=test.transformed$status)
plot(ROCit_lda, col = c(5,'grey50'),legend=FALSE)
lines(ROCit_ldaW$TPR~ROCit_ldaW$FPR, col=6)
legend("bottomright", col = c(75,6), c("LDA", "Weighted LDA"), lwd = 2)
model_weights <- ifelse(train.data$status == "failed",99, 1)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="accuracy", trControl=control,weights=model_weights)
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
ROCit_ldaW<- rocit(score=prob.lda$failed,class=test.transformed$status)
plot(ROCit_lda, col = c(5,'grey50'),legend=FALSE)
lines(ROCit_ldaW$TPR~ROCit_ldaW$FPR, col=6)
legend("bottomright", col = c(75,6), c("LDA", "Weighted LDA"), lwd = 2)
library(gbm)
install.packages("gbm")
calibrate.plot(test.transformed$status, prob.lda)
library(gbm)
calibrate.plot(test.transformed$status, prob.lda)
calibrate.plot(test.transformed$status, prob.lda$active)
calibrate.plot(test.transformed$status, prob.lda$failed)
calibrate.plot(test.transformed$status, prob.lr$failed)
#Calibration
calibrate.plot(testrf.transformed$status, prob.rf$failed)
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
nrow(train.data) / nrow(dfc)
calibration(prob.lda)
calibration(active,prob.lda)
calibration("active",prob.lda)
calibration(active.~,data = prob.lda)
calibration(active,data = prob.lda)
# b) LOGISTIC REGRESSION
calibration(active~.,data = prob.lda)
# b) LOGISTIC REGRESSION
calibration(active~failed,data = prob.lda)
calibration(obs ~ lda+ lr, data = testProbs)
testProbs <- data.frame(obs = test.transformed$status, lda = prob.lda$active, lr = prob.lr$active)
calibration(obs ~ lda+ lr, data = testProbs)
calPlotData <- calibration(obs ~ lda+ lr, data = testProbs)
xyplot(calPlotData, auto.key = list(columns = 2))
testProbs <- data.frame(obs = test.transformed$status, lda = prob.lda$active)#, lr = prob.lr$active)
calPlotData <- calibration(obs ~ lda+ lr, data = testProbs)
xyplot(calPlotData, auto.key = list(columns = 2))
calPlotData <- calibration(obs ~ lda, data = testProbs)
xyplot(calPlotData, auto.key = list(columns = 2))
#specificity
plot(specificity(prob.lda,test.transformed$status))
#specificity
plot(specificity(prob.lda$failed,test.transformed$status))
#specificity
plot(specificity(prodictions.lda$failed,test.transformed$status))
#specificity
plot(specificity(predictions.lda$failed,test.transformed$status))
#specificity
plot(specificity(predictions.lda,test.transformed$status))
#specificity
plot(Specificity(predictions.lda,test.transformed$status))
library(MLmetrics)
library(MLmeasures)
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
######DEFIN E CUSTOM METRICS
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
c(F1 = f1_val)
}
spec <- function(data, lev = NULL, model = NULL) {
spec_val <- Specificity(y_pred = dat$pred, y_true = data$obs, positive = "1")
c(SPEC = spec_val)
}
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="SPEC", trControl=control,weights=model_weights)
library(MLmetrics)
spec <- function(data, lev = NULL, model = NULL) {
spec_val <- MLmetrics::Specificity(y_pred = dat$pred, y_true = data$obs, positive = "1")
c(SPEC = spec_val)
}
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="SPEC", trControl=control,weights=model_weights)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="F1", trControl=control,weights=model_weights)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=F1, trControl=control,weights=model_weights)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
metric <- "Accuracy"
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
# confusion matrix
predictions.lda <- predict(fit.lda, test.transformed)
#specificity
plot(Specificity(predictions.lda,test.transformed$status))
#specificity
plot(Specificity(predictions.lda,test.transformed$status))
#specificity
plot(Specificity(predictions.lda%failed,test.transformed$status))
#specificity
plot(Specificity(predictions.lda$failed,test.transformed$status))
#specificity
plot(Specificity(prob.lda,test.transformed$status))
#specificity
plot(Specificity(prob.lda$failed,test.transformed$status))
#specificity
plot(Specificity(as.int(prob.lda$failed),test.transformed$status))
#specificity
plot(Specificity(as.factor(prob.lda$failed),test.transformed$status))
#specificity
plot(Specificity(prob.lda$failed,test.transformed$status))
prob.lda$failed
#specificity
Specificity(prediction.lda,test.transformed$status)
#specificity
Specificity(predictions.lda,test.transformed$status)
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
#specificity
Specificity(predictions.lda,test.transformed$status,positive = "active")
#specificity
Specificity(predictions.lda,test.transformed$status,positive = "failed")
lapply(prediction.lda,test.transformed$status)
lapply(c(prediction.lda,test.transformed$status))
for (i in c(1:10)){
print(i)
}
return( TN / (TN + FP))
myspecificty <- function(pred,obs, positive = NULL){
FP = 0;
TP = 0;
FN = 0;
TN = 0;
for (i in c(1:length(pred))){
if (pred[i] == positive){
if (obs[i] == positive){
TP = TP +1;
}
else{
FP =FP+1;
}
}
else{
if (obs[i] == positive){
FN = FN +1;
}
else{
TN =TN+1;
}
}
}
return( TN / (TN + FP))
}
#specificity
Specificity(predictions.lda,test.transformed$status,positive = "failed")
#specificity
mypecificity(predictions.lda,test.transformed$status,positive = "failed")
#specificity
myspecificity(predictions.lda,test.transformed$status,positive = "failed")
myspecificity <- function(pred,obs, positive = NULL){
FP = 0;
TP = 0;
FN = 0;
TN = 0;
for (i in c(1:length(pred))){
if (pred[i] == positive){
if (obs[i] == positive){
TP = TP +1;
}
else{
FP =FP+1;
}
}
else{
if (obs[i] == positive){
FN = FN +1;
}
else{
TN =TN+1;
}
}
}
return( TN / (TN + FP))
}
#specificity
myspecificity(predictions.lda,test.transformed$status,positive = "failed")
#specificity
myspecificity(predictions.lda,test.transformed$status,positive = "active")
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
spec <- function (data,
lev = NULL,
model = NULL) {
out <- myspecificity(data$obs - data$pred,
positive="active")
names(out) <- "SPEC"
out
}
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric="SPEC", trControl=control,weights=model_weights)
control <- trainControl(method="cv", number=10, summary_function="SPEC",classProbs = TRUE)
control <- trainControl(method="cv", number=10, summary_function=spec,classProbs = TRUE)
control <- trainControl(method="cv", number=10, summaryFunction=spec,classProbs = TRUE)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
spec <- function(data, lev = NULL, model = NULL) {
spec_val <- MLmetrics::Specificity(y_pred = dat$pred, y_true = data$obs, positive = "1")
c(SPEC = spec_val)
}
control <- trainControl(method="cv", number=10, summaryFunction=spec,classProbs = TRUE)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
spec_val <- MLmetrics::Specificity(y_pred = data$pred, y_true = data$obs, positive = "1")
spec <- function(data, lev = NULL, model = NULL) {
spec_val <- MLmetrics::Specificity(y_pred = data$pred, y_true = data$obs, positive = "1")
c(SPEC = spec_val)
}
control <- trainControl(method="cv", number=10, summaryFunction=spec,classProbs = TRUE)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
control <- trainControl(method="cv", number=10,classProbs = TRUE)
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
confusionMatrix(predictions.lr, as.factor(test.transformed$`status`),positive="failed")
confusionMatrix(predictions.lr, as.factor(test.transformed$`status`),positive="active")
confusionMatrix(predictions.rf, as.factor(testrf.transformed$`status`),positive="failed")
confusionMatrix(predictions.rf, as.factor(testrf.transformed$`status`),positive="active")
testProbs <- data.frame(obs = test.transformed$status, lda = prob.lda$active, lr = prob.lr$active,rf = prob.rf$active)#, lr = prob.lr$active)
calPlotData <- calibration(obs ~ lda + lr + rf, data = testProbs)
xyplot(calPlotData, auto.key = list(columns = 2))
xyplot(calPlotData, auto.key = list(columns = 3))
library(dplyr)
library(sjmisc)
library(magrittr)
library(caret)
library(ggplot2)
library(devtools)
library(tseries)
library(moments)
library(stringr)
library(gbm)
library(MLmetrics)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2011,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2011,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2012,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2012,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
train.active = train.data[train.data$status=='active',]
train.failed = train.data[train.data$status=='failed',]
train.active = train.active[sample(1:nrow(train.active), 50000),]
train.data <- rbind(train.failed,train.active)
nrow(train.data) / (nrow(train.data)+nrow(test.data))
table(train.data$status) / nrow(train.data)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2012,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2012,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2012,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2012,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
train.active = train.data[train.data$status=='active',]
train.failed = train.data[train.data$status=='failed',]
train.active = train.active[sample(1:nrow(train.active), nrow(train.failed)*2),]
train.data <- rbind(train.failed,train.active)
sample(1:nrow(dfc), nrow(test.data)*2)
nrow(train.data) / (nrow(train.data)+nrow(test.data))
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2013,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2013,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
train.active = train.data[train.data$status=='active',]
train.failed = train.data[train.data$status=='failed',]
train.active = train.active[sample(1:nrow(train.active), nrow(train.failed)*2),]
train.data <- rbind(train.failed,train.active)
sample(1:nrow(dfc), nrow(test.data)*2)
nrow(train.data) / (nrow(train.data)+nrow(test.data))
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
# Estimate preprocessing parameters
preproc.param <- train.data[,c(mostImportant,"status")] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data[,c(mostImportant,"status")])
test.transformed <- preproc.param %>% predict(test.data[,c(mostImportant,"status")])
######DEFIN E CUSTOM METRICS
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
c(F1 = f1_val)
}
spec <- function(data, lev = NULL, model = NULL) {
spec_val <- MLmetrics::Specificity(y_pred = data$pred, y_true = data$obs, positive = "1")
c(SPEC = spec_val)
}
control <- trainControl(method="cv", number=10,classProbs = TRUE)
metric <- "Accuracy"
# a) linear algorithms
set.seed(17)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights=model_weights)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control)
print(fit.lda)
# confusion matrix
predictions.lda <- predict(fit.lda, test.transformed)
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
#ROC curve
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
ROCit_lda<- rocit(score=prob.lda,class=test.transformed$status)
library(ROCit)
ROCit_lda<- rocit(score=prob.lda,class=test.transformed$status)
#ROC curve
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
ROCit_lda<- rocit(score=prob.lda,class=test.transformed$status)
dim(prob.lda)
dim(test.transformed)
ROCit_lda<- rocit(score=prob.lda$active,class=test.transformed$status)
plot(ROCit_lda, col = c(5,'grey50'),legend=FALSE)
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2012,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2012,attributes]
# Estimate preprocessing parameters
preproc.param <- trainrf.data[,attributes] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
trainrf.transformed <- preproc.param %>% predict(trainrf.data[,attributes])
testrf.transformed <- preproc.param %>% predict(testrf.data[,attributes])
trainrf.active = trainrf.data[trainrf.data$status=='active',]
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2012,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2012,attributes]
# Estimate preprocessing parameters
preproc.param <- trainrf.data[,attributes] %>%
preProcess(method = c("center", "scale"))
library(sjmisc)
library(magrittr)
library(caret)
library(ggplot2)
library(devtools)
library(tseries)
library(moments)
library(stringr)
library(gbm)
library(MLmetrics)
library(ROCit)
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2012,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2012,attributes]
# Estimate preprocessing parameters
preproc.param <- trainrf.data[,attributes] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
trainrf.transformed <- preproc.param %>% predict(trainrf.data[,attributes])
testrf.transformed <- preproc.param %>% predict(testrf.data[,attributes])
trainrf.active = trainrf.data[trainrf.data$status=='active',]
trainrf.failed = trainrf.data[trainrf.data$status=='failed',]
trainrf.active = trainrf.active[sample(1:nrow(trainrf.active), nrow(trainrf.failed)*2),]
trainrf.data <- rbind(trainrf.failed,trainrf.active)
nrow(trainrf.data) / (  nrow(trainrf.data)+nrow(testrf.data)
f
nrow(trainrf.data) / (  nrow(trainrf.data)+nrow(testrf.data)
)
fit.rf.U <- train(`status`~., data=trainrf.transformed, method="rf", metric="Kappa", trControl=control)
#confusion matrix
predictions.rf <- predict(fit.rf.U, testrf.transformed)
confusionMatrix(predictions.rf, as.factor(testrf.transformed$`status`),positive="active")
nrow(trainrf.data) / (nrow(trainrf.data)+nrow(test))
nrow(trainrf.data)
nrow(trainrf.data) / (nrow(trainrf.data)+nrow(testrf.data))
table(trainrf.data$status)
#ROC curve
prob.rf <- predict(fit.rf.U, testrf.transformed, type ='prob')
ROCit_rf <- rocit(score=prob.rf,class=testrf.transformed$status)
## All ROC curves
plot(ROCit_lda, col=c(2,"grey50"),legend = FALSE, YIndex = FALSE)
lines(ROCit_lr$TPR~ROCit_lr$FPR, col=3,lw=2)
lines(ROCit_rf$TPR~ROCit_rf$FPR, col=4,lw=2)
legend("bottomright", col = c(2,3,4), c("LDA","Logit Boost", "Ranfom Forest"), lwd = 2)
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control)
nrow(train.data) / (nrow(train.data)+nrow(test.data))
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
table(train.data$status)
metric <- "Kappa"
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control)
print(fit.lda)
# confusion matrix
predictions.lda <- predict(fit.lda, test.transformed)
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
#ROC curve
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
ROCit_lda<- rocit(score=prob.lda$active,class=test.transformed$status)
plot(ROCit_lda, col = c(5,'grey50'),legend=FALSE)
ROCit_lda<- rocit(score=prob.lda$failed,class=test.transformed$status)
## All ROC curves
plot(ROCit_lda, col=c(2,"grey50"),legend = FALSE, YIndex = FALSE)
lines(ROCit_lr$TPR~ROCit_lr$FPR, col=3,lw=2)
lines(ROCit_rf$TPR~ROCit_rf$FPR, col=4,lw=2)
legend("bottomright", col = c(2,3,4), c("LDA","Logit Boost", "Ranfom Forest"), lwd = 2)
install.packages('classifierplots')
library('classifierplots')
classifierplots_folder(testrf.transformed$status,prob.rf$failed, 'folder')
classifierplots_folder(as.factor(testrf.transformed$status),prob.rf$failed, 'folder')
classifierplots_folder(as.factor(testrf.transformed$active),prob.rf$failed, 'folder')
testrf.transformed$active
library(comprehenr)
library(rlist)
install.packages(rlist)
install.packages("rlist")
classifierplots_folder(ifelse(testrf.transformed$status == 'active',1,0),prob.rf$failed, 'folder')
classifierplots_plots(ifelse(testrf.transformed$status == 'active',1,0),prob.rf$failed)
classifierplots(ifelse(testrf.transformed$status == 'active',1,0),prob.rf$failed)
classifierplots(ifelse(testrf.transformed$status == 'active',1,0),prob.rf$active)
#ROC curves
prob.lr <- predict(fit.lr, test.transformed, type ='prob')
classifierplots(ifelse(test.transformed$status == 'active',1,0),prob.lr$active)
classifierplots(ifelse(test.transformed$status == 'active',1,0),prob.lda$active)
#from the plot hte only way to split the data temporlly is using >2018 as training, since 75% of data are from 2018
train.data = dfc[dfc$Incorporation.year < 2009,c(mostImportant,"status")]
test.data = dfc[dfc$Incorporation.year >= 2009,c(mostImportant,"status")]
nrow(train.data) / nrow(dfc)
table(train.data$status) / nrow(train.data)
table(test.data$status)/ nrow(test.data)
# Estimate preprocessing parameters
preproc.param <- train.data[,c(mostImportant,"status")] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data[,c(mostImportant,"status")])
test.transformed <- preproc.param %>% predict(test.data[,c(mostImportant,"status")])
control <- trainControl(method="cv", number=10,classProbs = TRUE)
metric <- "Kappa"
model_weights <- ifelse(train.data$status == "failed",99, 1)
View(train.data)
# Create model weights (they sum to one)
model_weights <- ifelse(imbal_train$Class == "Class1",
(1/table(train.data$status)[1]) * 0.5,
(1/table(train.data$status)[2]) * 0.5)
table(train.data$status)
# Create model weights (they sum to one)
model_weights <- ifelse(train.data$status == "active",
(1/table(train.data$status)[1]) * 0.5,
(1/table(train.data$status)[2]) * 0.5)
metric <- "Kappa"
fit.lda <- train(`status`~., data=train.transformed, method="lda", metric=metric, trControl=control,weights = model_weights)
print(fit.lda)
# confusion matrix
predictions.lda <- predict(fit.lda, test.transformed)
confusionMatrix(predictions.lda, as.factor(test.transformed$`status`),positive="active")
#ROC curve
prob.lda <- predict(fit.lda, test.transformed, type ='prob')
classifierplots(ifelse(test.transformed$status == 'active',1,0),prob.lda$active)
fit.lr <- train(`status`~., data=train.transformed, method="LogitBoost", metric="Kappa", trControl=control, weights = model_weights)
print(fit.rf)
#CONFUSION MATRIX
predictions.lr <- predict(fit.lr, test.transformed)
confusionMatrix(predictions.lr, as.factor(test.transformed$`status`),positive="active")
#ROC curves
prob.lr <- predict(fit.lr, test.transformed, type ='prob')
classifierplots(ifelse(test.transformed$status == 'active',1,0),prob.lr$active)
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2009,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2009,attributes]
# Estimate preprocessing parameters
preproc.param <- train.data[,c(mostImportant,"status")] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
trainrf.transformed <- preproc.param %>% predict(trainrf.data[,c(mostImportant,"status")])
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2009,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2009,attributes]
# Estimate preprocessing parameters
preproc.param <- train.data[,c(attributes,"status")] %>%
preProcess(method = c("center", "scale"))
###RANDOM FORES
trainrf.data = dfc[dfc$Incorporation.year < 2009,attributes]
testrf.data = dfc[dfc$Incorporation.year >= 2009,attributes]
# Estimate preprocessing parameters
preproc.param <- train.data[,attributes] %>%
preProcess(method = c("center", "scale"))
# Estimate preprocessing parameters
preproc.param <- trainrf.data[,attributes] %>%
preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
trainrf.transformed <- preproc.param %>% predict(trainrf.data[,attributes])
testrf.transformed <- preproc.param %>% predict(testrf.data[,attributes])
fit.rf <- train(`status`~., data=trainrf.transformed, method="rf", metric="Kappa", trControl=control, weights = model_weights)
#confusion matrix
predictions.rf <- predict(fit.rf, testrf.transformed)
confusionMatrix(predictions.rf, as.factor(testrf.transformed$`status`),positive="active")
#ROC curve
prob.rf <- predict(fit.rf, testrf.transformed, type ='prob')
classifierplots(ifelse(testrf.transformedrf$status == 'active',1,0),prob.rf$active)
classifierplots(ifelse(testrf.transformed$status == 'active',1,0),prob.rf$active)
## All ROC curves
plot(ROCit_lda, col=c(2,"grey50"),legend = FALSE, YIndex = FALSE)
lines(ROCit_lr$TPR~ROCit_lr$FPR, col=3,lw=2)
lines(ROCit_rf$TPR~ROCit_rf$FPR, col=4,lw=2)
legend("bottomright", col = c(2,3,4), c("LDA","Logit Boost", "Ranfom Forest"), lwd = 2)
testProbs <- data.frame(obs = test.transformed$status, lda = prob.lda$active, lr = prob.lr$active,rf = prob.rf$active)#, lr = prob.lr$active)
calPlotData <- calibration(obs ~ lda + lr + rf, data = testProbs)
xyplot(calPlotData, auto.key = list(columns = 3))
